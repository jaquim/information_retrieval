{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from math import sqrt, pow, ceil\n",
    "from decimal import Decimal\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import copy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_relevance = [0,1]\n",
    "cut_off = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Simulate Rankings of Relevance for E and P\n",
    "\n",
    "We make no assumptions regarding the identities of the documents in the rankings of E and P. This implies that the documents in the two rankings can be distinct or that they can overlap. To incorporate this into our simulation, we generate not only all possible pairs of relevance rankings, but for each pair we also generate all the possible ways that their documents could overlap. We implement this by assigning ids to the documents in each ranking, with overlap occurring if two documents in different rankings have the same id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_docs(docs1, docs2):  \n",
    "  \"\"\"\n",
    "  Take two lists of document relevance scores and generate all possible combinations of overlap\n",
    "  between the two lists.\n",
    "  :param docs1: ranking 1 relevance scores\n",
    "  :param docs2: ranking 2 relevance scores\n",
    "  :return: list of lists of tuples with all possible overlapping schemes\n",
    "  \"\"\"\n",
    "  \n",
    "  # Generate all possible document identifiers for irrelevant documents\n",
    "  zero_ids1 = list(range(docs1.count(0))) \n",
    "  if len(zero_ids1) == 0:\n",
    "    zero_ids2 = [[-1] * docs2.count(0)]  \n",
    "  else:\n",
    "    zero_ids2 = list(itertools.permutations(list(range(6)), docs2.count(0)))\n",
    "    zero_ids2 = sorted([[-1 if x>=docs1.count(0) else x for x in ids] for ids in zero_ids2])\n",
    "    zero_ids2 = list(ids for ids,_ in itertools.groupby(zero_ids2))  \n",
    "  \n",
    "  # Generate all possible document identifiers for relevant documents\n",
    "  one_ids1 = list(range(docs1.count(1))) \n",
    "  if len(one_ids1) == 0:\n",
    "    one_ids2 = [[-1] * docs2.count(1)]\n",
    "  else:\n",
    "    one_ids2 = list(itertools.permutations(list(range(6)), docs2.count(1)))\n",
    "    one_ids2 = sorted([[-1 if x>=docs1.count(1) else x for x in ids] for ids in one_ids2])\n",
    "    one_ids2 = list(ids for ids, _ in itertools.groupby(one_ids2))\n",
    "\n",
    "  # Label the documents of ranking 1\n",
    "  ranking1 = []\n",
    "  zero_count = one_count = 0\n",
    "  for doc in docs1:\n",
    "    if doc == 0:\n",
    "      ranking1.append((doc, zero_ids1[zero_count]))\n",
    "      zero_count += 1\n",
    "    else:\n",
    "      ranking1.append((doc, one_ids1[one_count]))\n",
    "      one_count += 1\n",
    "      \n",
    "  # Label the documents of ranking 2\n",
    "  labelled_rankings = []\n",
    "  for zero_ids in zero_ids2:\n",
    "    for one_ids in one_ids2:\n",
    "      ranking2 = []\n",
    "      zero_count = one_count = 0\n",
    "      for doc in docs2:\n",
    "        if doc == 0:\n",
    "          ranking2.append((doc, zero_ids[zero_count]))\n",
    "          zero_count += 1\n",
    "        else:\n",
    "          ranking2.append((doc, one_ids[one_count]))\n",
    "          one_count += 1        \n",
    "      labelled_rankings.append([ranking1, ranking2])\n",
    "\n",
    "  return labelled_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all possible pairs of relevance rankings\n",
    "system_e = list(map(list, itertools.product(binary_relevance, repeat=cut_off)))\n",
    "system_p = list(map(list, itertools.product(binary_relevance, repeat=cut_off)))\n",
    "\n",
    "# Expand the set of relevance rankings pairs to all possible overlapping configurations \n",
    "ranking_pairs = [list(ranking) for ranking in list(itertools.product(system_e, system_p))]\n",
    "labelled_rankings = [ranking for docs in ranking_pairs for ranking in identify_docs(docs[0], docs[1])]\n",
    "labelled_rankings[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Defining Expected Reciprocal Rank @ Cut-off (ERR@-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_relevance_to_probability = lambda pos_g, max_g: ((2**pos_g) - 1 ) / (2**max_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ERR(ranking, mapping=mapping_relevance_to_probability, n=cut_off):\n",
    "    p, err = 1, 0\n",
    "  \n",
    "    for r in range(0, n):\n",
    "        R = mapping_relevance_to_probability(ranking[r], 1)\n",
    "        err += (p * (R / (r + 1)))\n",
    "        p *= (1 - R)\n",
    "        \n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Interleaving\n",
    "\n",
    "The rankings produced by interleaving are cut-off at 3, since this is the maximum number of results a user can see. Using unique id labels, we make sure that there are no two identical documents in the interleaved rankings, as a part of both team draft and probabilistic interleaving algorithms.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': ['A', 0], '2': ['B', 0], '3': ['B', 0]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def team_draft_interleaving(ranking_input):\n",
    "    \"\"\"\n",
    "    Generates the interleaved ranking based on semi-stochastical choice of a \"team\" that drafts and on deterministical \n",
    "    choice of the draft pick.\n",
    "    Args: \n",
    "        ranking_input (list): two rankings proposed by each system with corresponding relevance and ids as tuples.\n",
    "    \n",
    "    returns a dict with ranking position as keys and list with team name and relevance as values\n",
    "    \"\"\"\n",
    "    #sorting by relevance:\n",
    "    ranking=copy.deepcopy(ranking_input)\n",
    "    ranking_a, ranking_b = ranking[0], ranking[1]\n",
    "    team_a, team_b = 0, 0\n",
    "    # I_dict has rankings number as keys and triplets of [ str \"Team_Name\", int relevance, int id]\n",
    "    I_dict = {\"1\": [],\"2\": [], \"3\": []} \n",
    "    for iteration in range(3):  # not the same stopping condition as in the paper, since there is a limit on the size of I\n",
    "        \n",
    "        #check to determine which team is drafting\n",
    "        if (team_a < team_b) or ((team_a == team_b) and np.random.randint(2) == 1):\n",
    "            #assignment \n",
    "            rank=ranking_a[0][0]\n",
    "            id_a=ranking_a[0][1]\n",
    "            I_dict[str(iteration+1)] = [\"A\",rank]\n",
    "            team_a += 1\n",
    "            #deletes duplicates\n",
    "            for i,(rank_b,id_b) in enumerate(ranking_b):\n",
    "                if id_a==id_b:\n",
    "                    del ranking_b[i]\n",
    "            #delete used ranking\n",
    "            del ranking_a[0]\n",
    "                    \n",
    "                        \n",
    "        else:\n",
    "            #assignment \n",
    "            rank=ranking_b[0][0]\n",
    "            id_b=ranking_b[0][1]\n",
    "            I_dict[str(iteration+1)] = [\"B\",rank]\n",
    "            team_b += 1\n",
    "            #deletes duplicates\n",
    "            for i,(rank_a,id_a) in enumerate(ranking_a):\n",
    "                if id_b==id_a:\n",
    "                    del ranking_a[i]\n",
    "            #delete used ranking\n",
    "            del ranking_b[0]\n",
    "                        \n",
    "    return I_dict\n",
    "team_draft_interleaving([[(0, 0), (0, 1), (0, 2)], [(0, -1), (0, 2), (0, -1)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': ['B', 0], '2': ['A', 0], '3': ['B', 0]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(d):\n",
    "    \"\"\"\n",
    "    Generates a softmax probability distribution based on the number of ranks and on their ranks.\n",
    "    Args:\n",
    "        d (list): a list with ranks\n",
    "    returns a dict with ranking position as keys and probability of adding a rank to the interleaving as values\n",
    "    \"\"\"\n",
    "    denominator=0\n",
    "    \n",
    "    length=len(d)\n",
    "    for i in range(length):\n",
    "        denominator+=1/((i+1)**tau)\n",
    "    soft={}\n",
    "    for i in range(length):\n",
    "        soft[i]=(1/((i+1)**tau)) / denominator\n",
    "    return soft\n",
    "\n",
    "def probabilistic_interleaving(ranking_input):\n",
    "    \"\"\"\n",
    "    Generates the interleaved ranking based on semi-stochastical choice of a \"team\" that drafts and on stochastic \n",
    "    choice of the draft pick.\n",
    "    Args: \n",
    "        ranking_input (list): two rankings proposed by each system with corresponding relevance and ids as tuples.\n",
    "    \n",
    "    returns a dict with ranking position as keys and list with team name and relevance as values\n",
    "    \"\"\"\n",
    "    ranking=copy.deepcopy(ranking_input)\n",
    "    #sorting by relevance:\n",
    "    ranking_a, ranking_b = ranking[0], ranking[1]\n",
    "    team_a, team_b = 0, 0\n",
    "    # I_dict has rankings number as keys and triplets of [ str \"Team_Name\", int relevance, int id]\n",
    "    I_dict = {\"1\": [],\"2\": [], \"3\": []} \n",
    "    for iteration in range(3):  # not the same stopping condition as in the paper, since there is a limit on the size of I\n",
    "        \n",
    "        #check to determine which team is drafting\n",
    "        if (team_a < team_b) or ((team_a == team_b) and np.random.randint(2) == 1):\n",
    "            #generating distribution and choosing\n",
    "            distribution=softmax(ranking_a)\n",
    "            choice=np.random.choice(list(distribution.keys()), p=list(distribution.values()))\n",
    "            #assignment\n",
    "            id_a=ranking_a[choice][1]\n",
    "            I_dict[str(iteration+1)] = [\"A\",ranking_a[choice][0]]\n",
    "            team_a += 1\n",
    "            #deleting duplicates\n",
    "            for i,(rank_b,id_b) in enumerate(ranking_b):\n",
    "                if id_a==id_b:\n",
    "                    del ranking_b[i]\n",
    "            \n",
    "            del ranking_a[choice]\n",
    "            \n",
    "                        \n",
    "        else:\n",
    "            #generating distribution and choosing\n",
    "            distribution=softmax(ranking_b)\n",
    "            choice=np.random.choice(list(distribution.keys()), p=list(distribution.values()))\n",
    "            #assignment\n",
    "            id_b=ranking_b[choice][1]\n",
    "            I_dict[str(iteration+1)] = [\"B\",ranking_b[choice][0]]\n",
    "            team_b += 1\n",
    "            #deleting duplicates\n",
    "            for i,(rank_a,id_a) in enumerate(ranking_a):\n",
    "                if id_b==id_a:\n",
    "                    del ranking_a[i]\n",
    "            \n",
    "            del ranking_b[choice]\n",
    "                        \n",
    "    return I_dict\n",
    "probabilistic_interleaving([[(0, 0), (0, 1), (0, 2)], [(0, -1), (0, 2), (0, -1)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Simulate User Clicks\n",
    "\n",
    "We implement a position-based model for clicking and train it with 100 iterations of the EM algorithm on the Yandex click log. We assume that documents clicked in a session can refer back to any of the queries in the same session, not just the most recent query (this assumption was communicated to us by our TA). After training the model we use the trained examination parameters for rank 1 to 3 and our own attractiveness parameters (0.1 for irrelevant documents and 0.9 for relevant documents) to simulate position-based clickes. We also implement the option of a random click as a sanity test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionBasedModel:\n",
    "\n",
    "    def __init__(self, stored_gammas=False):\n",
    "        self.gamma = defaultdict(float) if not stored_gammas else stored_gammas\n",
    "        self.alpha = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "    def train(self, training_file = \"YandexRelPredChallenge.txt\", iterations=10):\n",
    "\n",
    "        # Read data into dataframe\n",
    "        columns = [\"SessionID\", \"TimePassed\", \"TypeOfAction\", \"TargetID\", \"RegionID\", 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "        df = pd.read_csv(training_file, sep='\\t', header=None, names=columns)\n",
    "        print(\"Training with EM...\")\n",
    "\n",
    "        for i in range(iterations):\n",
    "\n",
    "             # Initialise sums and counts\n",
    "            gamma_count = defaultdict(lambda: 2)\n",
    "            alpha_count = defaultdict(lambda: defaultdict(lambda: 2))\n",
    "\n",
    "            gamma_sum = defaultdict(lambda: 1)\n",
    "            alpha_sum = defaultdict(lambda: defaultdict(lambda: 1))\n",
    "\n",
    "\n",
    "            # Iterate sessions\n",
    "            grouped = df.groupby(\"SessionID\")\n",
    "            for session_id, session_df in grouped:\n",
    "\n",
    "                # Extract session clicks\n",
    "                session_clicks = session_df[session_df[\"TypeOfAction\"] == \"C\"][\"TargetID\"].tolist()\n",
    "\n",
    "                # Iterate session queries\n",
    "                query_df = session_df[session_df[\"TypeOfAction\"] == \"Q\"]\n",
    "                for j, (index, row) in enumerate(query_df.iterrows()):\n",
    "\n",
    "                    query_id = row[\"TargetID\"]\n",
    "                    for rank in range(1, 11):  # from rank 1 to 10\n",
    "                        document_id = row[rank]\n",
    "\n",
    "                        # Determine what values should be added to the EM formula sums\n",
    "                        if document_id in session_clicks:\n",
    "                            gamma_value = alpha_value = 1\n",
    "                        else:\n",
    "                            gamma_value = (self.gamma[rank] * (1 - self.alpha[document_id][query_id])) / \\\n",
    "                                          (1 - self.gamma[rank] * self.alpha[document_id][query_id])\n",
    "                            alpha_value = ((1 - self.gamma[rank]) * self.alpha[document_id][query_id]) / \\\n",
    "                                          (1 - self.gamma[rank] * self.alpha[document_id][query_id])\n",
    "\n",
    "                        gamma_sum[rank] += gamma_value\n",
    "                        alpha_sum[document_id][query_id] += alpha_value\n",
    "\n",
    "                        gamma_count[rank] += 1\n",
    "                        alpha_count[document_id][query_id] += 1\n",
    "\n",
    "            # Update variables\n",
    "            for rank, param in self.gamma.items():\n",
    "                self.gamma[rank] = gamma_sum[rank] / gamma_count[rank]\n",
    "\n",
    "            for document_id, document_params in self.alpha.items():\n",
    "                for query_id, param in document_params.items():\n",
    "                    self.alpha[document_id][query_id] = alpha_sum[document_id][query_id] / alpha_count[document_id][query_id]\n",
    "\n",
    "            print(\"Completed iteration\", i+1)\n",
    "        print(\"Training complete\")\n",
    "\n",
    "    def click_prob(self, epsilon, rank, relevance):\n",
    "        # Calculate the probability of clicking on a document\n",
    "        attract = epsilon if relevance == 0 else 1-epsilon\n",
    "        click_prob = float(self.gamma[rank]) * attract\n",
    "        return click_prob\n",
    "\n",
    "    def click_doc(self, epsilon, rank, relevance):\n",
    "        # Decide whether a document is clicked on\n",
    "        random_number = random.uniform(0, 1)\n",
    "        return True if random_number < self.click_prob(epsilon, rank, relevance) else False\n",
    "    \n",
    "    def click_random(self, p=1./3.):\n",
    "        # Decide whether a document is clicked based on a random click model where each\n",
    "        # document is clicked with probability p\n",
    "        random_number = random.uniform(0, 1)\n",
    "        return True if random_number < p else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained gammas for 100 iterations\n",
    "gamma_100iterations = defaultdict(float, {1: 0.9998616618119641, \n",
    "                                          2: 0.6898705149478165, \n",
    "                                          3: 0.4737488629423336, \n",
    "                                          4: 0.35795929106830376, \n",
    "                                          5: 0.27471953217002687, \n",
    "                                          6: 0.22844461594225668, \n",
    "                                          7: 0.20124843776845872, \n",
    "                                          8: 0.17975265660898382, \n",
    "                                          9: 0.16342837688027292, \n",
    "                                          10: 0.16113079695259283})\n",
    "model = PositionBasedModel(stored_gammas=gamma_100iterations)\n",
    "# model.train(iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {1: 0.9998616618119641,\n",
       "             2: 0.6898705149478165,\n",
       "             3: 0.4737488629423336,\n",
       "             4: 0.35795929106830376,\n",
       "             5: 0.27471953217002687,\n",
       "             6: 0.22844461594225668,\n",
       "             7: 0.20124843776845872,\n",
       "             8: 0.17975265660898382,\n",
       "             9: 0.16342837688027292,\n",
       "             10: 0.16113079695259283})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8998754956307677"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.click_prob(epsilon=0.1, rank=1, relevance=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016113079695259283"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.click_prob(epsilon=0.1, rank=10, relevance=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.click_doc(epsilon=0.1, rank=1, relevance=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.click_doc(epsilon=0.1, rank=2, relevance=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1_alpha = stats.norm.ppf(0.95) # alpha = 0.05\n",
    "z1_beta = stats.norm.ppf(0.9) # beta = 0.1\n",
    "def compute_impressions(p1, p0=0.5, z1_alpha=z1_alpha, z1_beta=z1_beta):\n",
    "\n",
    "    \"\"\"\"\n",
    "    Compute the amount of impressions needed according to the z-test.\n",
    "    \n",
    "    Args: \n",
    "        p1: proportion of wins to be tested for\n",
    "        p0: standard value to test against\n",
    "        z1_alpha: signifiance value\n",
    "        z1_beta: significance value for a power of 1-beta\n",
    "    \n",
    "    returns the amount of impressions n to needed to prove significance.\n",
    "    \"\"\"\n",
    "    \n",
    "    null = z1_alpha * (sqrt(p0 * (1 - p0)))\n",
    "    alternative = z1_beta * (sqrt(p1 * (1 - p1)))\n",
    "    n = ceil(pow(((null + alternative) / (p1 - p0)), 2))\n",
    "    \n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_analysis(interleaving_method, click_model, ranking, n_simulations, random=False, include_ties=False):\n",
    "    wins_e, wins_p, ties = 0, 0, 0\n",
    "    \n",
    "    for i in range(n_simulations):\n",
    "        click_e, click_p = 0, 0\n",
    "        interleaving = interleaving_method(ranking)\n",
    "        generated_list = [(value[0], value[1]) for value in interleaving.values()]\n",
    "        \n",
    "        for rank, (team, relevance) in enumerate(generated_list):\n",
    "            if not random:\n",
    "                click = click_model.click_doc(epsilon=0.1, rank=rank+1, relevance=relevance)\n",
    "            else:\n",
    "                click = click_model.click_random()\n",
    "            \n",
    "            if click:\n",
    "                if 'A' in team:\n",
    "                    click_e += 1\n",
    "                else:\n",
    "                    click_p += 1\n",
    "        \n",
    "        if click_e > click_p:\n",
    "            \n",
    "            wins_e += 1\n",
    "            \n",
    "        elif click_p > click_e:\n",
    "            \n",
    "            wins_p +=1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            wins_p += 1\n",
    "            wins_e += 1\n",
    "                    \n",
    "    if include_ties:\n",
    "        proportion_e = wins_e / (wins_e + wins_p  + ties)\n",
    "    else:\n",
    "        proportion_e = wins_e / (wins_e + wins_p)\n",
    "        \n",
    "    impressions = compute_impressions(proportion_e)\n",
    "            \n",
    "    return impressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_significance(interleaving_method, table, labelled_rankings=labelled_rankings, metric=ERR, click_model=model, n_simulations=10**3, random=False, include_ties=False):\n",
    "    \n",
    "    \"\"\"\"\n",
    "       \n",
    "    Construct a table for intervals of the ERR metric. For each of these intervals, computes the mean, min and max. \n",
    "    \n",
    "    Args: \n",
    "        interleaving_method: interleaving function of interest\n",
    "        table: dict to bin and store results in\n",
    "        labelled_rankings: dataset that contains all the possible ranking pairs\n",
    "        metric: metric to compute the accuracy of a given ranking for\n",
    "        click_model: model for generating user clicks\n",
    "        n_simulations: amount of simulations desired\n",
    "        random: whether to generate random clicks or not\n",
    "        include_ties: specify how to compute the proportion of wins\n",
    "    \n",
    "    returns a binned pandas dataframe for given intervals of ERR and statistics from computed impressions\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    x = len(labelled_rankings)\n",
    "    \n",
    "    for i, ranking in enumerate(labelled_rankings):\n",
    "        if i % 100 == 0:\n",
    "            print('processing %s/%s'%(i,x))\n",
    "        system_e = ranking[0]\n",
    "        system_p = ranking[1]\n",
    "        \n",
    "        relevances_e = [e[0] for e in system_e]\n",
    "        relevances_p = [p[0] for p in system_p]\n",
    "        \n",
    "        metric_e = metric(relevances_e)\n",
    "        metric_p = metric(relevances_p)\n",
    "        \n",
    "        delta_metric = metric_e - metric_p\n",
    "        \n",
    "        if delta_metric >= 0:\n",
    "            \n",
    "            try:\n",
    "                impressions = power_analysis(interleaving_method, click_model, ranking, n_simulations, random=random, include_ties=include_ties)\n",
    "                table[delta_metric].append(impressions)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    statistics_table = dict()\n",
    "    table = pd.DataFrame.from_dict(table, orient='index')   \n",
    "    bins =  np.insert(np.insert(np.arange(0.1, 1, 0.1), 0, 0.05, axis=0), 10, 0.95, axis=0)\n",
    "    groups = table.groupby(pd.cut(table.index, bins, right=False), axis=0) #.agg(['min', 'mean', 'max'])\n",
    "    for interval, data in groups:\n",
    "        values_in_bin = [value for value in data.values.flatten().tolist() if not np.isnan(value)]\n",
    "        if values_in_bin:\n",
    "            statistics = dict(min=np.min(values_in_bin), median=np.median(values_in_bin), max=np.max(values_in_bin))\n",
    "            statistics_table[interval] = statistics\n",
    "        else:\n",
    "            statistics = dict(min=0, median=0, max=0)\n",
    "            statistics_table[interval] = statistics\n",
    "        \n",
    "    \n",
    "    statistics_table = pd.DataFrame.from_dict(statistics_table, orient='index')\n",
    "        \n",
    "    return statistics_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0/688\n",
      "processing 100/688\n",
      "processing 200/688\n",
      "processing 300/688\n",
      "processing 400/688\n",
      "processing 500/688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaq/Documents/NLP/venv/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 600/688\n",
      "                  min    median          max\n",
      "[0.05, 0.1)   17176.0  125270.5   16893215.0\n",
      "[0.1, 0.2)     5744.0   53493.5   29914894.0\n",
      "[0.2, 0.3)     4880.0  136251.0   67524768.0\n",
      "[0.3, 0.4)    10140.0  113657.5    7348578.0\n",
      "[0.4, 0.5)    15120.0  524383.0    4202279.0\n",
      "[0.5, 0.6)    12508.0  151049.0  270580243.0\n",
      "[0.6, 0.7)   100140.0  168528.5     423732.0\n",
      "[0.7, 0.8)        0.0       0.0          0.0\n",
      "[0.8, 0.9)        0.0       0.0          0.0\n",
      "[0.9, 0.95)       0.0       0.0          0.0\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &       min &    median &          max \\\\\n",
      "\\midrule\n",
      "[0.05, 0.1) &   17176.0 &  125270.5 &   16893215.0 \\\\\n",
      "[0.1, 0.2)  &    5744.0 &   53493.5 &   29914894.0 \\\\\n",
      "[0.2, 0.3)  &    4880.0 &  136251.0 &   67524768.0 \\\\\n",
      "[0.3, 0.4)  &   10140.0 &  113657.5 &    7348578.0 \\\\\n",
      "[0.4, 0.5)  &   15120.0 &  524383.0 &    4202279.0 \\\\\n",
      "[0.5, 0.6)  &   12508.0 &  151049.0 &  270580243.0 \\\\\n",
      "[0.6, 0.7)  &  100140.0 &  168528.5 &     423732.0 \\\\\n",
      "[0.7, 0.8)  &       0.0 &       0.0 &          0.0 \\\\\n",
      "[0.8, 0.9)  &       0.0 &       0.0 &          0.0 \\\\\n",
      "[0.9, 0.95) &       0.0 &       0.0 &          0.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "team_draft_table = defaultdict(lambda: list())\n",
    "probabilistic_table = defaultdict(lambda: list())\n",
    "\n",
    "team_draft_table = calculate_significance(team_draft_interleaving, team_draft_table, n_simulations=10**4, random=True)\n",
    "\n",
    "print(team_draft_table)\n",
    "print(team_draft_table.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0/688\n",
      "processing 100/688\n",
      "processing 200/688\n",
      "processing 300/688\n",
      "processing 400/688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaq/Documents/NLP/venv/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 500/688\n",
      "processing 600/688\n"
     ]
    }
   ],
   "source": [
    "probabilistic_table = calculate_significance(probabilistic_interleaving, probabilistic_table, n_simulations=10**4, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  min    median          max\n",
      "[0.05, 0.1)   16788.0  100101.5    5296479.0\n",
      "[0.1, 0.2)    12610.0  336289.5   64669924.0\n",
      "[0.2, 0.3)     4790.0  122447.5  261227563.0\n",
      "[0.3, 0.4)     6561.0  137252.0   64481794.0\n",
      "[0.4, 0.5)     9745.0  212379.0   29426062.0\n",
      "[0.5, 0.6)    16910.0  170378.0  265597112.0\n",
      "[0.6, 0.7)   157769.0  173208.5     188648.0\n",
      "[0.7, 0.8)        0.0       0.0          0.0\n",
      "[0.8, 0.9)        0.0       0.0          0.0\n",
      "[0.9, 0.95)       0.0       0.0          0.0\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &       min &    median &          max \\\\\n",
      "\\midrule\n",
      "[0.05, 0.1) &   16788.0 &  100101.5 &    5296479.0 \\\\\n",
      "[0.1, 0.2)  &   12610.0 &  336289.5 &   64669924.0 \\\\\n",
      "[0.2, 0.3)  &    4790.0 &  122447.5 &  261227563.0 \\\\\n",
      "[0.3, 0.4)  &    6561.0 &  137252.0 &   64481794.0 \\\\\n",
      "[0.4, 0.5)  &    9745.0 &  212379.0 &   29426062.0 \\\\\n",
      "[0.5, 0.6)  &   16910.0 &  170378.0 &  265597112.0 \\\\\n",
      "[0.6, 0.7)  &  157769.0 &  173208.5 &     188648.0 \\\\\n",
      "[0.7, 0.8)  &       0.0 &       0.0 &          0.0 \\\\\n",
      "[0.8, 0.9)  &       0.0 &       0.0 &          0.0 \\\\\n",
      "[0.9, 0.95) &       0.0 &       0.0 &          0.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(probabilistic_table)\n",
    "print(probabilistic_table.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PBM Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0/688\n",
      "processing 100/688\n",
      "processing 200/688\n",
      "processing 300/688\n",
      "processing 400/688\n",
      "processing 500/688\n",
      "processing 600/688\n",
      "              min  median        max\n",
      "[0.05, 0.1)  26.0    97.0  8007956.0\n",
      "[0.1, 0.2)   11.0    92.5  9567947.0\n",
      "[0.2, 0.3)    6.0    12.0     2263.0\n",
      "[0.3, 0.4)    5.0    11.0      118.0\n",
      "[0.4, 0.5)    5.0    10.0       12.0\n",
      "[0.5, 0.6)    5.0     6.0       11.0\n",
      "[0.6, 0.7)    5.0     5.0        6.0\n",
      "[0.7, 0.8)    0.0     0.0        0.0\n",
      "[0.8, 0.9)    0.0     0.0        0.0\n",
      "[0.9, 0.95)   0.0     0.0        0.0\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &   min &  median &        max \\\\\n",
      "\\midrule\n",
      "[0.05, 0.1) &  26.0 &    97.0 &  8007956.0 \\\\\n",
      "[0.1, 0.2)  &  11.0 &    92.5 &  9567947.0 \\\\\n",
      "[0.2, 0.3)  &   6.0 &    12.0 &     2263.0 \\\\\n",
      "[0.3, 0.4)  &   5.0 &    11.0 &      118.0 \\\\\n",
      "[0.4, 0.5)  &   5.0 &    10.0 &       12.0 \\\\\n",
      "[0.5, 0.6)  &   5.0 &     6.0 &       11.0 \\\\\n",
      "[0.6, 0.7)  &   5.0 &     5.0 &        6.0 \\\\\n",
      "[0.7, 0.8)  &   0.0 &     0.0 &        0.0 \\\\\n",
      "[0.8, 0.9)  &   0.0 &     0.0 &        0.0 \\\\\n",
      "[0.9, 0.95) &   0.0 &     0.0 &        0.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "team_draft_table = defaultdict(lambda: list())\n",
    "probabilistic_table = defaultdict(lambda: list())\n",
    "\n",
    "# if not 'random=True' is passed, the PBM model is used\n",
    "team_draft_table = calculate_significance(team_draft_interleaving, team_draft_table, n_simulations=10**4)\n",
    "\n",
    "print(team_draft_table)\n",
    "print(team_draft_table.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0/688\n",
      "processing 100/688\n",
      "processing 200/688\n",
      "processing 300/688\n",
      "processing 400/688\n",
      "processing 500/688\n",
      "processing 600/688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaq/Documents/NLP/venv/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# if not 'random=True' is passed, the PBM model is used\n",
    "probabilistic_table = calculate_significance(probabilistic_interleaving, probabilistic_table, n_simulations=10**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              min  median       max\n",
      "[0.05, 0.1)  22.0    78.0   25962.0\n",
      "[0.1, 0.2)   20.0    38.5     493.0\n",
      "[0.2, 0.3)   11.0    21.0  742296.0\n",
      "[0.3, 0.4)    8.0    13.0     119.0\n",
      "[0.4, 0.5)    7.0    10.5      17.0\n",
      "[0.5, 0.6)    6.0     7.0      12.0\n",
      "[0.6, 0.7)    6.0     6.0       6.0\n",
      "[0.7, 0.8)    0.0     0.0       0.0\n",
      "[0.8, 0.9)    0.0     0.0       0.0\n",
      "[0.9, 0.95)   0.0     0.0       0.0\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &   min &  median &       max \\\\\n",
      "\\midrule\n",
      "[0.05, 0.1) &  22.0 &    78.0 &   25962.0 \\\\\n",
      "[0.1, 0.2)  &  20.0 &    38.5 &     493.0 \\\\\n",
      "[0.2, 0.3)  &  11.0 &    21.0 &  742296.0 \\\\\n",
      "[0.3, 0.4)  &   8.0 &    13.0 &     119.0 \\\\\n",
      "[0.4, 0.5)  &   7.0 &    10.5 &      17.0 \\\\\n",
      "[0.5, 0.6)  &   6.0 &     7.0 &      12.0 \\\\\n",
      "[0.6, 0.7)  &   6.0 &     6.0 &       6.0 \\\\\n",
      "[0.7, 0.8)  &   0.0 &     0.0 &       0.0 \\\\\n",
      "[0.8, 0.9)  &   0.0 &     0.0 &       0.0 \\\\\n",
      "[0.9, 0.95) &   0.0 &     0.0 &       0.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(probabilistic_table)\n",
    "print(probabilistic_table.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "##### Team-Draft Interleaving, Random click model\n",
    "\n",
    "\n",
    "| $$\\Delta ERR@3$$ \t|      min \t|   median \t|         max \t|\n",
    "|------------------\t|---------:\t|---------:\t|------------:\t|\n",
    "| [0.05, 0.1)      \t|  17176.0 \t| 125270.5 \t|  16893215.0 \t|\n",
    "| [0.1, 0.2)       \t|   5744.0 \t|  53493.5 \t|  29914894.0 \t|\n",
    "| [0.2, 0.3)       \t|   4880.0 \t| 136251.0 \t|  67524768.0 \t|\n",
    "| [0.3, 0.4)       \t|  10140.0 \t| 113657.5 \t|   7348578.0 \t|\n",
    "| [0.4, 0.5)       \t|  15120.0 \t| 524383.0 \t|   4202279.0 \t|\n",
    "| [0.5, 0.6)       \t|  12508.0 \t| 151049.0 \t| 270580243.0 \t|\n",
    "| [0.6, 0.7)       \t| 100140.0 \t| 168528.5 \t|    423732.0 \t|\n",
    "| [0.7, 0.8)       \t|      0.0 \t|      0.0 \t|         0.0 \t|\n",
    "| [0.8, 0.9)       \t|      0.0 \t|      0.0 \t|         0.0 \t|\n",
    "| [0.9, 0.95)      \t|      0.0 \t|      0.0 \t|         0.0 \t|\n",
    "\n",
    "##### Probabilistic Interleaving, Random click model\n",
    "\n",
    "| $$\\Delta ERR@3$$ \t|      min \t|   median \t|         max \t|\n",
    "|------------------\t|---------:\t|---------:\t|------------:\t|\n",
    "| [0.05, 0.1)      \t|  16788.0 \t| 100101.5 \t|   5296479.0 \t|\n",
    "| [0.1, 0.2)       \t|  12610.0 \t| 336289.5 \t|  64669924.0 \t|\n",
    "| [0.2, 0.3)       \t|   4790.0 \t| 122447.5 \t| 261227563.0 \t|\n",
    "| [0.3, 0.4)       \t|   6561.0 \t| 137252.0 \t|  64481794.0 \t|\n",
    "| [0.4, 0.5)       \t|   9745.0 \t| 212379.0 \t|  29426062.0 \t|\n",
    "| [0.5, 0.6)       \t|  16910.0 \t| 170378.0 \t| 265597112.0 \t|\n",
    "| [0.6, 0.7)       \t| 157769.0 \t| 173208.5 \t|    188648.0 \t|\n",
    "| [0.7, 0.8)       \t|      0.0 \t|      0.0 \t|         0.0 \t|\n",
    "| [0.8, 0.9)       \t|      0.0 \t|      0.0 \t|         0.0 \t|\n",
    "| [0.9, 0.95)      \t|      0.0 \t|      0.0 \t|         0.0 \t|\n",
    "\n",
    "\n",
    "\n",
    "##### Team-Draft Interleaving, PBM\n",
    "\n",
    "| $$\\Delta ERR@3$$  \t|  min \t| median \t|       max \t|\n",
    "|-------------\t|-----:\t|-------:\t|----------:\t|\n",
    "| [0.05, 0.1) \t| 26.0 \t|   97.0 \t| 8007956.0 \t|\n",
    "| [0.1, 0.2)  \t| 11.0 \t|   92.5 \t| 9567947.0 \t|\n",
    "| [0.2, 0.3)  \t|  6.0 \t|   12.0 \t|    2263.0 \t|\n",
    "| [0.3, 0.4)  \t|  5.0 \t|   11.0 \t|     118.0 \t|\n",
    "| [0.4, 0.5)  \t|  5.0 \t|   10.0 \t|      12.0 \t|\n",
    "| [0.5, 0.6)  \t|  5.0 \t|    6.0 \t|      11.0 \t|\n",
    "| [0.6, 0.7)  \t|  5.0 \t|    5.0 \t|       6.0 \t|\n",
    "| [0.7, 0.8)  \t|  0.0 \t|    0.0 \t|       0.0 \t|\n",
    "| [0.8, 0.9)  \t|  0.0 \t|    0.0 \t|       0.0 \t|\n",
    "| [0.9, 0.95) \t|  0.0 \t|    0.0 \t|       0.0 \t|\n",
    "\n",
    "\n",
    "\n",
    "##### Probabilistic Interleaving, PBM\n",
    "\n",
    "|  $$\\Delta ERR@3 $$ \t|  min \t| median \t|      max \t|\n",
    "|---------------------------------\t|-----:\t|-------:\t|---------:\t|\n",
    "| [0.05, 0.1)                     \t| 22.0 \t|   78.0 \t|  25962.0 \t|\n",
    "| [0.1, 0.2)                      \t| 20.0 \t|   38.5 \t|    493.0 \t|\n",
    "| [0.2, 0.3)                      \t| 11.0 \t|   21.0 \t| 742296.0 \t|\n",
    "| [0.3, 0.4)                      \t|  8.0 \t|   13.0 \t|    119.0 \t|\n",
    "| [0.4, 0.5)                      \t|  7.0 \t|   10.5 \t|     17.0 \t|\n",
    "| [0.5, 0.6)                      \t|  6.0 \t|    7.0 \t|     12.0 \t|\n",
    "| [0.6, 0.7)                      \t|  6.0 \t|    6.0 \t|      6.0 \t|\n",
    "| [0.7, 0.8)                      \t|  0.0 \t|    0.0 \t|      0.0 \t|\n",
    "| [0.8, 0.9)                      \t|  0.0 \t|    0.0 \t|      0.0 \t|\n",
    "| [0.9, 0.95)                     \t|  0.0 \t|    0.0 \t|      0.0 \t|\n",
    "\n",
    "\n",
    "\n",
    "#### Observations\n",
    "\n",
    "From the tables we notice that the smaller the difference is in the computed ERR, the more impressions are need for this bin to prove significance for this difference. For bins that have higher margins, we indeed observe an (exponential) decreasing fashion of the impressions needed. Intuitively this makes sense, due to the smaller that this difference is, the more impressions are needed to show significance.\n",
    "\n",
    "\n",
    "#### Improvements\n",
    "\n",
    "Out of several possible models for simulation of user click behaviour, in our experiments, we have only implemented two models - i.e. a random click model and a position based model (PBM). The underlying assumption to the latter is that a user may generate several clicks per query, and for a click to occur, several parameters (attractiveness) are to be estimated to compute the probability of the click. Other methods to estimate these parameters, or to even include other parameters that model for additional assumptions about the accuracy of the user behaviour, are perhaps possible improvements to our experimental design. Yet another model, that discounts for documents shown after documents of high relevance, the so called cascaded models, provide yet another assumption about the accuracy of the user behaviour. Considering these cascaded models may also provide an imporivement to our experimental design.\n",
    "\n",
    "\n",
    "Different metrics are in circulation to compute the quality for, for a given ranking in offline evaluation. Whereas we have now only considered the metric of choice only for a rankings up until a cut-off at the third position, a more realistic approach would be to increase the cut-off position. Whereas this is not done now due to lack of computational and time resources, actually doing so, ensures to model more accurately a real world setting.\n",
    "\n",
    "We have only considered a binary relevance judgements. By incorporating a finer grained judgement for relevances of documents, we expect to model for more accurate user feedback in assessing the users' opinion on a shown ranking. Hence, this would be an expected improvement in our experimental design.\n",
    "\n",
    "\n",
    "More advanced statistic test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
